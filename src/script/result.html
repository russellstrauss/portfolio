<div class=\"reading\"><p>Markov Chains are directed graphs with special properties. In a Markov Chain $\\Omega$, every node in the graph represents a state.$$ \\Omega = (e, v) $$</p><p>The weighted, directed edges represent the probability from transitioning from one state to another. $$w(e) = edge(i, j) = \\mathbb{P}(i \\rightarrow j)$$ The weight of the edge from vertex $i$ to vertex $j$ is equal to the probability of transition from state $i$ to state $j$. We can discretize time $t = \\{0, 1, 2, ..., t_n\\}$ such that you can think of $t$ in seconds, minutes, or whichever unit.</p><p>The probability of staying at the same state is represented by a self-loop edge back onto the state itself. Since the probability of either staying at the same state or moving to a different state is 100%, all edges leaving a vertex must sum to 1. For all $v \\in \\Omega$$$\\sum_{j}P(i,j) = 1$$</p><p>The adjacency matrix for this graph is called transition matrix $P$, size $n$ x $n$, where each element $P[i, j]$ represents the probability of moving from state $i$ to $j$ in a single step. Any element $P[i, j] = 0$ if $edge(i, j) \\notin \\Omega$. We can calculate probabilities of moving from state to state in more than one time step by raising the transition matrix to powers of $P$ equal to the $t$ number of steps.$$P^{t}(i, j) = P(i \\rightarrow j)$$</p><p>Now if instead of calculating the transition matrix for a few steps in discrete time, it is possible to calculate $P^t$ as the limit of $t$ trends infinitely. $$\\lim_{t \\to \\infty} P^{t} = \\begin{bmatrix} \\pi \\\\ \\pi \\\\ \\vdots \\\\ \\pi\\end{bmatrix} $$ So every row of our transition matrix $P$ has converged to eigenvector $\\pi$. This is known as the stationary distribution.</p><p>Our defining property of the stationary distribution is $$\\pi P=\\pi$$</p><p>So what does this mean? It means that if you calculate $P^t$ for large enough $t$, regardless of which state you start within, the probability that you will be at state $j$ at time $t$ is $\\pi(j)$ for every $v \\in \\Omega$. Of course this is only helpful if the stationary distribution is unique. If it does not converge, how can we be sure that the probability of moving from state $i$ to $j$ is contained within our $\\pi$ vector if there could be more than possible solution for the distribution?</p><p>However, there are certain properties of Markov Chains that can ensure we reach a unique stationary distribution. If we can do this, then we can determine the respective probabilities of being in any state at a given time. How quickly we can calculate this distribution is known as the mxing time of the MArkov Chain.</p><p>For there to be a unique stationary distribution $\\pi$, the Markov Chain must be ergodic. The conditions for ergodicity are:</p><ol><li>Aperiodicity</li><li>Irreducibility</li></ol><h3>Aperiodicity</h3><h3>Irreducibility</h3></div>